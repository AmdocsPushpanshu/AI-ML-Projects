{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence Learning Assignment 1 - Question 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efbFg6FIbHhQ",
        "outputId": "b2f13a2b-08ff-444e-e9d3-87c9702ceef1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc9MywytvQSG"
      },
      "source": [
        "# **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkmLCMRNvXfb",
        "outputId": "2e5545eb-e986-4fd7-8b5a-544cf9e98a30"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9f74QF50XDJ"
      },
      "source": [
        "# **Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU-W9-Vc0aLy",
        "outputId": "255748f6-d785-4e09-b455-82e2462ffce7"
      },
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/Sequence Learning Assignments/CRF_POS_dataset.csv', encoding = \"ISO-8859-1\")\n",
        "print(df.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(567007, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "UlCQz8UU0qY3",
        "outputId": "e255fc1a-725d-4d79-c289-56045a02d980"
      },
      "source": [
        "df.head(30)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Num</th>\n",
              "      <th>Word</th>\n",
              "      <th>Tag_POS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>A</td>\n",
              "      <td>DT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>37-year-old</td>\n",
              "      <td>JJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>woman</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>has</td>\n",
              "      <td>VBZ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>become</td>\n",
              "      <td>VBN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NaN</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>13th</td>\n",
              "      <td>JJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NaN</td>\n",
              "      <td>person</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NaN</td>\n",
              "      <td>in</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Egypt</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NaN</td>\n",
              "      <td>to</td>\n",
              "      <td>TO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NaN</td>\n",
              "      <td>die</td>\n",
              "      <td>VB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>NaN</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>NaN</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>NaN</td>\n",
              "      <td>H5N1</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>NaN</td>\n",
              "      <td>strain</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>NaN</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>NaN</td>\n",
              "      <td>bird</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>NaN</td>\n",
              "      <td>flu</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>NaN</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2.0</td>\n",
              "      <td>Nadia</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Mohammed</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Abdel</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Hafez</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>NaN</td>\n",
              "      <td>died</td>\n",
              "      <td>VBD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>NaN</td>\n",
              "      <td>in</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>NaN</td>\n",
              "      <td>a</td>\n",
              "      <td>DT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>NaN</td>\n",
              "      <td>hospital</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>NaN</td>\n",
              "      <td>in</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Cairo</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Num         Word Tag_POS\n",
              "0   1.0            A      DT\n",
              "1   NaN  37-year-old      JJ\n",
              "2   NaN        woman      NN\n",
              "3   NaN          has     VBZ\n",
              "4   NaN       become     VBN\n",
              "5   NaN          the      DT\n",
              "6   NaN         13th      JJ\n",
              "7   NaN       person      NN\n",
              "8   NaN           in      IN\n",
              "9   NaN        Egypt     NNP\n",
              "10  NaN           to      TO\n",
              "11  NaN          die      VB\n",
              "12  NaN           of      IN\n",
              "13  NaN          the      DT\n",
              "14  NaN         H5N1     NNP\n",
              "15  NaN       strain      NN\n",
              "16  NaN           of      IN\n",
              "17  NaN         bird      NN\n",
              "18  NaN          flu      NN\n",
              "19  NaN            .       .\n",
              "20  2.0        Nadia     NNP\n",
              "21  NaN     Mohammed     NNP\n",
              "22  NaN        Abdel     NNP\n",
              "23  NaN        Hafez     NNP\n",
              "24  NaN         died     VBD\n",
              "25  NaN           in      IN\n",
              "26  NaN            a      DT\n",
              "27  NaN     hospital      NN\n",
              "28  NaN           in      IN\n",
              "29  NaN        Cairo     NNP"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdqc90VI0mSg",
        "outputId": "5743d51a-6e78-42c6-eb44-ebf9c93bffb8"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Num        541078\n",
              "Word            0\n",
              "Tag_POS         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmYrosCz0zyi"
      },
      "source": [
        "df = df.fillna(method='ffill')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "hexahiRX012N",
        "outputId": "29ad3c3e-e7e1-4419-fd8a-3d5805420beb"
      },
      "source": [
        "df.head(30)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Num</th>\n",
              "      <th>Word</th>\n",
              "      <th>Tag_POS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>A</td>\n",
              "      <td>DT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>37-year-old</td>\n",
              "      <td>JJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>woman</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>has</td>\n",
              "      <td>VBZ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>become</td>\n",
              "      <td>VBN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.0</td>\n",
              "      <td>13th</td>\n",
              "      <td>JJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.0</td>\n",
              "      <td>person</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.0</td>\n",
              "      <td>in</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Egypt</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.0</td>\n",
              "      <td>to</td>\n",
              "      <td>TO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.0</td>\n",
              "      <td>die</td>\n",
              "      <td>VB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.0</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.0</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.0</td>\n",
              "      <td>H5N1</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.0</td>\n",
              "      <td>strain</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.0</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.0</td>\n",
              "      <td>bird</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.0</td>\n",
              "      <td>flu</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.0</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2.0</td>\n",
              "      <td>Nadia</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2.0</td>\n",
              "      <td>Mohammed</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2.0</td>\n",
              "      <td>Abdel</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2.0</td>\n",
              "      <td>Hafez</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2.0</td>\n",
              "      <td>died</td>\n",
              "      <td>VBD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2.0</td>\n",
              "      <td>in</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2.0</td>\n",
              "      <td>a</td>\n",
              "      <td>DT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2.0</td>\n",
              "      <td>hospital</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2.0</td>\n",
              "      <td>in</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2.0</td>\n",
              "      <td>Cairo</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Num         Word Tag_POS\n",
              "0   1.0            A      DT\n",
              "1   1.0  37-year-old      JJ\n",
              "2   1.0        woman      NN\n",
              "3   1.0          has     VBZ\n",
              "4   1.0       become     VBN\n",
              "5   1.0          the      DT\n",
              "6   1.0         13th      JJ\n",
              "7   1.0       person      NN\n",
              "8   1.0           in      IN\n",
              "9   1.0        Egypt     NNP\n",
              "10  1.0           to      TO\n",
              "11  1.0          die      VB\n",
              "12  1.0           of      IN\n",
              "13  1.0          the      DT\n",
              "14  1.0         H5N1     NNP\n",
              "15  1.0       strain      NN\n",
              "16  1.0           of      IN\n",
              "17  1.0         bird      NN\n",
              "18  1.0          flu      NN\n",
              "19  1.0            .       .\n",
              "20  2.0        Nadia     NNP\n",
              "21  2.0     Mohammed     NNP\n",
              "22  2.0        Abdel     NNP\n",
              "23  2.0        Hafez     NNP\n",
              "24  2.0         died     VBD\n",
              "25  2.0           in      IN\n",
              "26  2.0            a      DT\n",
              "27  2.0     hospital      NN\n",
              "28  2.0           in      IN\n",
              "29  2.0        Cairo     NNP"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoCu_emM1G2C",
        "outputId": "b1c15d3d-4ae2-476b-fec0-cb911460d32f"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Num        0\n",
              "Word       0\n",
              "Tag_POS    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyfuADgX2uNt",
        "outputId": "8b952898-7a90-4faa-dcdb-3ef5ea29bad6"
      },
      "source": [
        "df.Num.nunique(), df.Word.nunique(), df.Tag_POS.nunique()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25929, 26398, 42)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZBPpKFV3IFl",
        "outputId": "fa126c1e-88b6-4765-a4a3-700d662ccbcd"
      },
      "source": [
        "df.groupby('Tag_POS').size()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tag_POS\n",
              "$         625\n",
              ",       17704\n",
              ".       25852\n",
              ":         426\n",
              ";         104\n",
              "CC      12737\n",
              "CD      13538\n",
              "DT      52987\n",
              "EX        343\n",
              "FW          1\n",
              "IN      65465\n",
              "JJ      42521\n",
              "JJR      1640\n",
              "JJS      1683\n",
              "LRB       393\n",
              "MD       3759\n",
              "NN      78624\n",
              "NNP     71316\n",
              "NNPS     1362\n",
              "NNS     40985\n",
              "PDT        84\n",
              "POS      6094\n",
              "PRP      7236\n",
              "PRP$     4741\n",
              "RB      10889\n",
              "RBR       577\n",
              "RBS       160\n",
              "RP       1324\n",
              "RRB       394\n",
              "TO      12393\n",
              "UH         13\n",
              "VB      12939\n",
              "VBD     21334\n",
              "VBG     10313\n",
              "VBN     17492\n",
              "VBP      8683\n",
              "VBZ     13512\n",
              "WDT      2018\n",
              "WP       1384\n",
              "WP$        57\n",
              "WRB      1182\n",
              "``       2123\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze-yLiiy5OT4"
      },
      "source": [
        "# **Prepare Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q4RQkf--96d"
      },
      "source": [
        "**Clean Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lBAmpd0-8na"
      },
      "source": [
        "def preprocess_text(str):\n",
        "  import re\n",
        "  import nltk\n",
        "  from nltk.tokenize import word_tokenize\n",
        "  \n",
        "  # Remove and replace \"'\", \"--\", \"-\",\"[\",\"]\" by \" \"  \n",
        "  str = re.sub(r'[\\]\\[\\-\\--\\/.\\'\\,(;:)\\\\\"!?]',r' ', str, flags=re.MULTILINE)\n",
        "  \n",
        "  words = word_tokenize(str)\n",
        "  \n",
        "  clean_text = ''\n",
        "  \n",
        "  for word in words:\n",
        "    clean_text = clean_text+' '+word\n",
        "  \n",
        "  return clean_text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pu5QEwEaAIa5",
        "outputId": "eda764b1-0dea-42dc-ea99-9547999f65f6"
      },
      "source": [
        "'''\n",
        "\n",
        "for i in range(len(df)):\n",
        "  df.loc[i, 'Word'] = preprocess_text(df.loc[i, 'Word'])\n",
        "\n",
        "print(df.head(30))\n",
        "'''"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n\\nfor i in range(len(df)):\\n  df.loc[i, 'Word'] = preprocess_text(df.loc[i, 'Word'])\\n\\nprint(df.head(30))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY49uFs85S35"
      },
      "source": [
        "**Get Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-06taUe5VAH",
        "outputId": "b9809f45-81e9-4c9f-fea0-c956d3ca639f"
      },
      "source": [
        "agg_func = lambda s: [(w, p) for w, p in zip(s['Word'].values.tolist(), s['Tag_POS'].values.tolist())]\n",
        "grouped = df.groupby('Num').apply(agg_func)\n",
        "sentences = [s for s in grouped]\n",
        "grouped.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Num\n",
              "1.0    [(A, DT), (37-year-old, JJ), (woman, NN), (has...\n",
              "2.0    [(Nadia, NNP), (Mohammed, NNP), (Abdel, NNP), ...\n",
              "3.0    [(Health, NNP), (officials, NNS), (initially, ...\n",
              "4.0    [(The, DT), (woman, NN), (raised, VBD), (poult...\n",
              "5.0    [(Health, NNP), (officials, NNS), (announced, ...\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3FbiqRd8_bb"
      },
      "source": [
        "**Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti35sCpD9Eqt",
        "outputId": "628e8de1-cedc-411a-c7ff-9486e1dab571"
      },
      "source": [
        "print(type(sentences)), print(len(sentences)), print(sentences[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "25929\n",
            "[('A', 'DT'), ('37-year-old', 'JJ'), ('woman', 'NN'), ('has', 'VBZ'), ('become', 'VBN'), ('the', 'DT'), ('13th', 'JJ'), ('person', 'NN'), ('in', 'IN'), ('Egypt', 'NNP'), ('to', 'TO'), ('die', 'VB'), ('of', 'IN'), ('the', 'DT'), ('H5N1', 'NNP'), ('strain', 'NN'), ('of', 'IN'), ('bird', 'NN'), ('flu', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wARKkRcuC-6N"
      },
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "    \n",
        "    features = {\n",
        "        'bias': 1.0, \n",
        "        'word.lower()': word.lower(), \n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [postag for token, postag in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag in sent]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1aYp5xSDmAs",
        "outputId": "140990af-b22c-42aa-c09c-253f0ac2e033"
      },
      "source": [
        "print(sentences[0])\n",
        "sent2features(sentences[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('A', 'DT'), ('37-year-old', 'JJ'), ('woman', 'NN'), ('has', 'VBZ'), ('become', 'VBN'), ('the', 'DT'), ('13th', 'JJ'), ('person', 'NN'), ('in', 'IN'), ('Egypt', 'NNP'), ('to', 'TO'), ('die', 'VB'), ('of', 'IN'), ('the', 'DT'), ('H5N1', 'NNP'), ('strain', 'NN'), ('of', 'IN'), ('bird', 'NN'), ('flu', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'+1:postag': 'JJ',\n",
              "  '+1:postag[:2]': 'JJ',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': '37-year-old',\n",
              "  'BOS': True,\n",
              "  'bias': 1.0,\n",
              "  'postag': 'DT',\n",
              "  'postag[:2]': 'DT',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': True,\n",
              "  'word.isupper()': True,\n",
              "  'word.lower()': 'a',\n",
              "  'word[-2:]': 'A',\n",
              "  'word[-3:]': 'A'},\n",
              " {'+1:postag': 'NN',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'woman',\n",
              "  '-1:postag': 'DT',\n",
              "  '-1:postag[:2]': 'DT',\n",
              "  '-1:word.istitle()': True,\n",
              "  '-1:word.isupper()': True,\n",
              "  '-1:word.lower()': 'a',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'JJ',\n",
              "  'postag[:2]': 'JJ',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': '37-year-old',\n",
              "  'word[-2:]': 'ld',\n",
              "  'word[-3:]': 'old'},\n",
              " {'+1:postag': 'VBZ',\n",
              "  '+1:postag[:2]': 'VB',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'has',\n",
              "  '-1:postag': 'JJ',\n",
              "  '-1:postag[:2]': 'JJ',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': '37-year-old',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NN',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'woman',\n",
              "  'word[-2:]': 'an',\n",
              "  'word[-3:]': 'man'},\n",
              " {'+1:postag': 'VBN',\n",
              "  '+1:postag[:2]': 'VB',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'become',\n",
              "  '-1:postag': 'NN',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'woman',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'VBZ',\n",
              "  'postag[:2]': 'VB',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'has',\n",
              "  'word[-2:]': 'as',\n",
              "  'word[-3:]': 'has'},\n",
              " {'+1:postag': 'DT',\n",
              "  '+1:postag[:2]': 'DT',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'the',\n",
              "  '-1:postag': 'VBZ',\n",
              "  '-1:postag[:2]': 'VB',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'has',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'VBN',\n",
              "  'postag[:2]': 'VB',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'become',\n",
              "  'word[-2:]': 'me',\n",
              "  'word[-3:]': 'ome'},\n",
              " {'+1:postag': 'JJ',\n",
              "  '+1:postag[:2]': 'JJ',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': '13th',\n",
              "  '-1:postag': 'VBN',\n",
              "  '-1:postag[:2]': 'VB',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'become',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'DT',\n",
              "  'postag[:2]': 'DT',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'the',\n",
              "  'word[-2:]': 'he',\n",
              "  'word[-3:]': 'the'},\n",
              " {'+1:postag': 'NN',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'person',\n",
              "  '-1:postag': 'DT',\n",
              "  '-1:postag[:2]': 'DT',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'the',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'JJ',\n",
              "  'postag[:2]': 'JJ',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': '13th',\n",
              "  'word[-2:]': 'th',\n",
              "  'word[-3:]': '3th'},\n",
              " {'+1:postag': 'IN',\n",
              "  '+1:postag[:2]': 'IN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'in',\n",
              "  '-1:postag': 'JJ',\n",
              "  '-1:postag[:2]': 'JJ',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': '13th',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NN',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'person',\n",
              "  'word[-2:]': 'on',\n",
              "  'word[-3:]': 'son'},\n",
              " {'+1:postag': 'NNP',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': True,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'egypt',\n",
              "  '-1:postag': 'NN',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'person',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'IN',\n",
              "  'postag[:2]': 'IN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'in',\n",
              "  'word[-2:]': 'in',\n",
              "  'word[-3:]': 'in'},\n",
              " {'+1:postag': 'TO',\n",
              "  '+1:postag[:2]': 'TO',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'to',\n",
              "  '-1:postag': 'IN',\n",
              "  '-1:postag[:2]': 'IN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'in',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NNP',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': True,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'egypt',\n",
              "  'word[-2:]': 'pt',\n",
              "  'word[-3:]': 'ypt'},\n",
              " {'+1:postag': 'VB',\n",
              "  '+1:postag[:2]': 'VB',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'die',\n",
              "  '-1:postag': 'NNP',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': True,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'egypt',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'TO',\n",
              "  'postag[:2]': 'TO',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'to',\n",
              "  'word[-2:]': 'to',\n",
              "  'word[-3:]': 'to'},\n",
              " {'+1:postag': 'IN',\n",
              "  '+1:postag[:2]': 'IN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'of',\n",
              "  '-1:postag': 'TO',\n",
              "  '-1:postag[:2]': 'TO',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'to',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'VB',\n",
              "  'postag[:2]': 'VB',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'die',\n",
              "  'word[-2:]': 'ie',\n",
              "  'word[-3:]': 'die'},\n",
              " {'+1:postag': 'DT',\n",
              "  '+1:postag[:2]': 'DT',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'the',\n",
              "  '-1:postag': 'VB',\n",
              "  '-1:postag[:2]': 'VB',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'die',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'IN',\n",
              "  'postag[:2]': 'IN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'of',\n",
              "  'word[-2:]': 'of',\n",
              "  'word[-3:]': 'of'},\n",
              " {'+1:postag': 'NNP',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': True,\n",
              "  '+1:word.isupper()': True,\n",
              "  '+1:word.lower()': 'h5n1',\n",
              "  '-1:postag': 'IN',\n",
              "  '-1:postag[:2]': 'IN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'of',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'DT',\n",
              "  'postag[:2]': 'DT',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'the',\n",
              "  'word[-2:]': 'he',\n",
              "  'word[-3:]': 'the'},\n",
              " {'+1:postag': 'NN',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'strain',\n",
              "  '-1:postag': 'DT',\n",
              "  '-1:postag[:2]': 'DT',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'the',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NNP',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': True,\n",
              "  'word.isupper()': True,\n",
              "  'word.lower()': 'h5n1',\n",
              "  'word[-2:]': 'N1',\n",
              "  'word[-3:]': '5N1'},\n",
              " {'+1:postag': 'IN',\n",
              "  '+1:postag[:2]': 'IN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'of',\n",
              "  '-1:postag': 'NNP',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': True,\n",
              "  '-1:word.isupper()': True,\n",
              "  '-1:word.lower()': 'h5n1',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NN',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'strain',\n",
              "  'word[-2:]': 'in',\n",
              "  'word[-3:]': 'ain'},\n",
              " {'+1:postag': 'NN',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'bird',\n",
              "  '-1:postag': 'NN',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'strain',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'IN',\n",
              "  'postag[:2]': 'IN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'of',\n",
              "  'word[-2:]': 'of',\n",
              "  'word[-3:]': 'of'},\n",
              " {'+1:postag': 'NN',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'flu',\n",
              "  '-1:postag': 'IN',\n",
              "  '-1:postag[:2]': 'IN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'of',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NN',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'bird',\n",
              "  'word[-2:]': 'rd',\n",
              "  'word[-3:]': 'ird'},\n",
              " {'+1:postag': '.',\n",
              "  '+1:postag[:2]': '.',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': '.',\n",
              "  '-1:postag': 'NN',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'bird',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NN',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'flu',\n",
              "  'word[-2:]': 'lu',\n",
              "  'word[-3:]': 'flu'},\n",
              " {'-1:postag': 'NN',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'flu',\n",
              "  'EOS': True,\n",
              "  'bias': 1.0,\n",
              "  'postag': '.',\n",
              "  'postag[:2]': '.',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': '.',\n",
              "  'word[-2:]': '.',\n",
              "  'word[-3:]': '.'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01sVNvUoD20p",
        "outputId": "b7e18ace-c342-44fc-9866-3c62b8230ab5"
      },
      "source": [
        "print(sentences[0])\n",
        "print(sent2labels(sentences[0]))\n",
        "print(sent2tokens(sentences[0]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('A', 'DT'), ('37-year-old', 'JJ'), ('woman', 'NN'), ('has', 'VBZ'), ('become', 'VBN'), ('the', 'DT'), ('13th', 'JJ'), ('person', 'NN'), ('in', 'IN'), ('Egypt', 'NNP'), ('to', 'TO'), ('die', 'VB'), ('of', 'IN'), ('the', 'DT'), ('H5N1', 'NNP'), ('strain', 'NN'), ('of', 'IN'), ('bird', 'NN'), ('flu', 'NN'), ('.', '.')]\n",
            "['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'TO', 'VB', 'IN', 'DT', 'NNP', 'NN', 'IN', 'NN', 'NN', '.']\n",
            "['A', '37-year-old', 'woman', 'has', 'become', 'the', '13th', 'person', 'in', 'Egypt', 'to', 'die', 'of', 'the', 'H5N1', 'strain', 'of', 'bird', 'flu', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS5syY0I4Xfk"
      },
      "source": [
        "# **Build Conditional Random Fields - CRF Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HzI-4Da4cst",
        "outputId": "7aca955e-98f0-4435-92f5-f23a8b2ec427"
      },
      "source": [
        "! pip install sklearn_crfsuite\n",
        "! pip install eli5"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.7/dist-packages (0.3.6)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (0.9.7)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (0.8.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (4.41.1)\n",
            "Requirement already satisfied: eli5 in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from eli5) (2.11.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from eli5) (1.15.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from eli5) (0.10.1)\n",
            "Requirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.7/dist-packages (from eli5) (21.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from eli5) (0.22.2.post1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from eli5) (0.8.9)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from eli5) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from eli5) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->eli5) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->eli5) (2.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S82J6ENT4k1V"
      },
      "source": [
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuvypKeKHRnU"
      },
      "source": [
        "**Create X and y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBAYLD644mxE"
      },
      "source": [
        "X = [sent2features(s) for s in sentences]\n",
        "y = [sent2labels(s) for s in sentences]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1evDdTzTHIDp",
        "outputId": "a14fd262-0529-4738-ef94-9b2fe838cdae"
      },
      "source": [
        "print(sentences[0])\n",
        "print(y[0])\n",
        "X[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('A', 'DT'), ('37-year-old', 'JJ'), ('woman', 'NN'), ('has', 'VBZ'), ('become', 'VBN'), ('the', 'DT'), ('13th', 'JJ'), ('person', 'NN'), ('in', 'IN'), ('Egypt', 'NNP'), ('to', 'TO'), ('die', 'VB'), ('of', 'IN'), ('the', 'DT'), ('H5N1', 'NNP'), ('strain', 'NN'), ('of', 'IN'), ('bird', 'NN'), ('flu', 'NN'), ('.', '.')]\n",
            "['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'TO', 'VB', 'IN', 'DT', 'NNP', 'NN', 'IN', 'NN', 'NN', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'+1:postag': 'JJ',\n",
              "  '+1:postag[:2]': 'JJ',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': '37-year-old',\n",
              "  'BOS': True,\n",
              "  'bias': 1.0,\n",
              "  'postag': 'DT',\n",
              "  'postag[:2]': 'DT',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': True,\n",
              "  'word.isupper()': True,\n",
              "  'word.lower()': 'a',\n",
              "  'word[-2:]': 'A',\n",
              "  'word[-3:]': 'A'},\n",
              " {'+1:postag': 'NN',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'woman',\n",
              "  '-1:postag': 'DT',\n",
              "  '-1:postag[:2]': 'DT',\n",
              "  '-1:word.istitle()': True,\n",
              "  '-1:word.isupper()': True,\n",
              "  '-1:word.lower()': 'a',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'JJ',\n",
              "  'postag[:2]': 'JJ',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': '37-year-old',\n",
              "  'word[-2:]': 'ld',\n",
              "  'word[-3:]': 'old'},\n",
              " {'+1:postag': 'VBZ',\n",
              "  '+1:postag[:2]': 'VB',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'has',\n",
              "  '-1:postag': 'JJ',\n",
              "  '-1:postag[:2]': 'JJ',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': '37-year-old',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NN',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'woman',\n",
              "  'word[-2:]': 'an',\n",
              "  'word[-3:]': 'man'},\n",
              " {'+1:postag': 'VBN',\n",
              "  '+1:postag[:2]': 'VB',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'become',\n",
              "  '-1:postag': 'NN',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'woman',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'VBZ',\n",
              "  'postag[:2]': 'VB',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'has',\n",
              "  'word[-2:]': 'as',\n",
              "  'word[-3:]': 'has'},\n",
              " {'+1:postag': 'DT',\n",
              "  '+1:postag[:2]': 'DT',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'the',\n",
              "  '-1:postag': 'VBZ',\n",
              "  '-1:postag[:2]': 'VB',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'has',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'VBN',\n",
              "  'postag[:2]': 'VB',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'become',\n",
              "  'word[-2:]': 'me',\n",
              "  'word[-3:]': 'ome'},\n",
              " {'+1:postag': 'JJ',\n",
              "  '+1:postag[:2]': 'JJ',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': '13th',\n",
              "  '-1:postag': 'VBN',\n",
              "  '-1:postag[:2]': 'VB',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'become',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'DT',\n",
              "  'postag[:2]': 'DT',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'the',\n",
              "  'word[-2:]': 'he',\n",
              "  'word[-3:]': 'the'},\n",
              " {'+1:postag': 'NN',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'person',\n",
              "  '-1:postag': 'DT',\n",
              "  '-1:postag[:2]': 'DT',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'the',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'JJ',\n",
              "  'postag[:2]': 'JJ',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': '13th',\n",
              "  'word[-2:]': 'th',\n",
              "  'word[-3:]': '3th'},\n",
              " {'+1:postag': 'IN',\n",
              "  '+1:postag[:2]': 'IN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'in',\n",
              "  '-1:postag': 'JJ',\n",
              "  '-1:postag[:2]': 'JJ',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': '13th',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NN',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'person',\n",
              "  'word[-2:]': 'on',\n",
              "  'word[-3:]': 'son'},\n",
              " {'+1:postag': 'NNP',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': True,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'egypt',\n",
              "  '-1:postag': 'NN',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'person',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'IN',\n",
              "  'postag[:2]': 'IN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'in',\n",
              "  'word[-2:]': 'in',\n",
              "  'word[-3:]': 'in'},\n",
              " {'+1:postag': 'TO',\n",
              "  '+1:postag[:2]': 'TO',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'to',\n",
              "  '-1:postag': 'IN',\n",
              "  '-1:postag[:2]': 'IN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'in',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NNP',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': True,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'egypt',\n",
              "  'word[-2:]': 'pt',\n",
              "  'word[-3:]': 'ypt'},\n",
              " {'+1:postag': 'VB',\n",
              "  '+1:postag[:2]': 'VB',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'die',\n",
              "  '-1:postag': 'NNP',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': True,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'egypt',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'TO',\n",
              "  'postag[:2]': 'TO',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'to',\n",
              "  'word[-2:]': 'to',\n",
              "  'word[-3:]': 'to'},\n",
              " {'+1:postag': 'IN',\n",
              "  '+1:postag[:2]': 'IN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'of',\n",
              "  '-1:postag': 'TO',\n",
              "  '-1:postag[:2]': 'TO',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'to',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'VB',\n",
              "  'postag[:2]': 'VB',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'die',\n",
              "  'word[-2:]': 'ie',\n",
              "  'word[-3:]': 'die'},\n",
              " {'+1:postag': 'DT',\n",
              "  '+1:postag[:2]': 'DT',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'the',\n",
              "  '-1:postag': 'VB',\n",
              "  '-1:postag[:2]': 'VB',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'die',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'IN',\n",
              "  'postag[:2]': 'IN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'of',\n",
              "  'word[-2:]': 'of',\n",
              "  'word[-3:]': 'of'},\n",
              " {'+1:postag': 'NNP',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': True,\n",
              "  '+1:word.isupper()': True,\n",
              "  '+1:word.lower()': 'h5n1',\n",
              "  '-1:postag': 'IN',\n",
              "  '-1:postag[:2]': 'IN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'of',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'DT',\n",
              "  'postag[:2]': 'DT',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'the',\n",
              "  'word[-2:]': 'he',\n",
              "  'word[-3:]': 'the'},\n",
              " {'+1:postag': 'NN',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'strain',\n",
              "  '-1:postag': 'DT',\n",
              "  '-1:postag[:2]': 'DT',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'the',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NNP',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': True,\n",
              "  'word.isupper()': True,\n",
              "  'word.lower()': 'h5n1',\n",
              "  'word[-2:]': 'N1',\n",
              "  'word[-3:]': '5N1'},\n",
              " {'+1:postag': 'IN',\n",
              "  '+1:postag[:2]': 'IN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'of',\n",
              "  '-1:postag': 'NNP',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': True,\n",
              "  '-1:word.isupper()': True,\n",
              "  '-1:word.lower()': 'h5n1',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NN',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'strain',\n",
              "  'word[-2:]': 'in',\n",
              "  'word[-3:]': 'ain'},\n",
              " {'+1:postag': 'NN',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'bird',\n",
              "  '-1:postag': 'NN',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'strain',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'IN',\n",
              "  'postag[:2]': 'IN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'of',\n",
              "  'word[-2:]': 'of',\n",
              "  'word[-3:]': 'of'},\n",
              " {'+1:postag': 'NN',\n",
              "  '+1:postag[:2]': 'NN',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': 'flu',\n",
              "  '-1:postag': 'IN',\n",
              "  '-1:postag[:2]': 'IN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'of',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NN',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'bird',\n",
              "  'word[-2:]': 'rd',\n",
              "  'word[-3:]': 'ird'},\n",
              " {'+1:postag': '.',\n",
              "  '+1:postag[:2]': '.',\n",
              "  '+1:word.istitle()': False,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:word.lower()': '.',\n",
              "  '-1:postag': 'NN',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'bird',\n",
              "  'bias': 1.0,\n",
              "  'postag': 'NN',\n",
              "  'postag[:2]': 'NN',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': 'flu',\n",
              "  'word[-2:]': 'lu',\n",
              "  'word[-3:]': 'flu'},\n",
              " {'-1:postag': 'NN',\n",
              "  '-1:postag[:2]': 'NN',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:word.lower()': 'flu',\n",
              "  'EOS': True,\n",
              "  'bias': 1.0,\n",
              "  'postag': '.',\n",
              "  'postag[:2]': '.',\n",
              "  'word.isdigit()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isupper()': False,\n",
              "  'word.lower()': '.',\n",
              "  'word[-2:]': '.',\n",
              "  'word[-3:]': '.'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb3r5TzkO8CL",
        "outputId": "4fd8bf65-36aa-404b-f63a-8af0a4f83762"
      },
      "source": [
        "new_classes = df['Tag_POS'].unique().tolist()\n",
        "new_classes.sort()\n",
        "print(new_classes)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['$', ',', '.', ':', ';', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LRB', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'RRB', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVzmdMp6OcBk"
      },
      "source": [
        "**Train using cross_validate_predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fEudIwcOgux"
      },
      "source": [
        "crf_cvp = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v933guEiOqO8",
        "outputId": "7231ee8a-ddcd-41ab-d90d-47c672aba53a"
      },
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "y_pred_cvp = cross_val_predict(crf_cvp, X, y, cv=3)\n",
        "metrics.flat_f1_score(y, y_pred_cvp, average='weighted', labels=new_classes)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999973545625818"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_UrWUVvO4Wf",
        "outputId": "fa6262b0-288a-4424-e371-3e5d561e027d"
      },
      "source": [
        "print(metrics.flat_classification_report(y, y_pred_cvp, labels = new_classes))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           $       1.00      1.00      1.00       625\n",
            "           ,       1.00      1.00      1.00     17704\n",
            "           .       1.00      1.00      1.00     25852\n",
            "           :       1.00      1.00      1.00       426\n",
            "           ;       1.00      1.00      1.00       104\n",
            "          CC       1.00      1.00      1.00     12737\n",
            "          CD       1.00      1.00      1.00     13538\n",
            "          DT       1.00      1.00      1.00     52987\n",
            "          EX       1.00      1.00      1.00       343\n",
            "          FW       0.00      0.00      0.00         1\n",
            "          IN       1.00      1.00      1.00     65465\n",
            "          JJ       1.00      1.00      1.00     42521\n",
            "         JJR       1.00      1.00      1.00      1640\n",
            "         JJS       1.00      1.00      1.00      1683\n",
            "         LRB       1.00      1.00      1.00       393\n",
            "          MD       1.00      1.00      1.00      3759\n",
            "          NN       1.00      1.00      1.00     78624\n",
            "         NNP       1.00      1.00      1.00     71316\n",
            "        NNPS       1.00      1.00      1.00      1362\n",
            "         NNS       1.00      1.00      1.00     40985\n",
            "         PDT       1.00      1.00      1.00        84\n",
            "         POS       1.00      1.00      1.00      6094\n",
            "         PRP       1.00      1.00      1.00      7236\n",
            "        PRP$       1.00      1.00      1.00      4741\n",
            "          RB       1.00      1.00      1.00     10889\n",
            "         RBR       1.00      1.00      1.00       577\n",
            "         RBS       1.00      1.00      1.00       160\n",
            "          RP       1.00      1.00      1.00      1324\n",
            "         RRB       1.00      1.00      1.00       394\n",
            "          TO       1.00      1.00      1.00     12393\n",
            "          UH       1.00      1.00      1.00        13\n",
            "          VB       1.00      1.00      1.00     12939\n",
            "         VBD       1.00      1.00      1.00     21334\n",
            "         VBG       1.00      1.00      1.00     10313\n",
            "         VBN       1.00      1.00      1.00     17492\n",
            "         VBP       1.00      1.00      1.00      8683\n",
            "         VBZ       1.00      1.00      1.00     13512\n",
            "         WDT       1.00      1.00      1.00      2018\n",
            "          WP       1.00      1.00      1.00      1384\n",
            "         WP$       1.00      1.00      1.00        57\n",
            "         WRB       1.00      1.00      1.00      1182\n",
            "          ``       1.00      1.00      1.00      2123\n",
            "\n",
            "    accuracy                           1.00    567007\n",
            "   macro avg       0.98      0.98      0.98    567007\n",
            "weighted avg       1.00      1.00      1.00    567007\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88FvRhKLHVv1"
      },
      "source": [
        "**Split Train and Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W35ovMGHZFm"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy8WZQqEIa7q",
        "outputId": "eaf8b909-3437-4350-c440-51f26fa555a5"
      },
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_states=None, all_possible_transitions=True,\n",
              "    averaging=None, c=None, c1=0.1, c2=0.1, calibration_candidates=None,\n",
              "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
              "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
              "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
              "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
              "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1WRy-tAJDAr",
        "outputId": "bcb8c8a5-af1b-4c72-e7c3-e458a01825a7"
      },
      "source": [
        "y_pred = crf.predict(X_test)\n",
        "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=new_classes)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999919664081423"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM8jYtCrJUSi",
        "outputId": "2b38c6e3-daca-4bc9-9a07-2261505c5a59"
      },
      "source": [
        "i = 63\n",
        "y = y_pred[i]\n",
        "X = [wordfeatures['word.lower()'] for wordfeatures in X_test[i]]\n",
        "print(' '.join(X))\n",
        "for word, entity in zip(X,y):\n",
        "  if entity != 'O':\n",
        "    print(word,entity)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "if returned to peru , he faces charges of corruption and of authorizing death squads .\n",
            "if IN\n",
            "returned VBN\n",
            "to TO\n",
            "peru NNP\n",
            ", ,\n",
            "he PRP\n",
            "faces VBZ\n",
            "charges NNS\n",
            "of IN\n",
            "corruption NN\n",
            "and CC\n",
            "of IN\n",
            "authorizing VBG\n",
            "death NN\n",
            "squads NNS\n",
            ". .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmBgnGx7Jg40",
        "outputId": "b7e43712-0341-4406-dc21-64c1bb54b530"
      },
      "source": [
        "print(metrics.flat_classification_report(y_test, y_pred, labels = new_classes))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           $       1.00      1.00      1.00       184\n",
            "           ,       1.00      1.00      1.00      5747\n",
            "           .       1.00      1.00      1.00      8540\n",
            "           :       1.00      1.00      1.00       153\n",
            "           ;       1.00      1.00      1.00        31\n",
            "          CC       1.00      1.00      1.00      4162\n",
            "          CD       1.00      1.00      1.00      4456\n",
            "          DT       1.00      1.00      1.00     17464\n",
            "          EX       1.00      1.00      1.00       113\n",
            "          FW       0.00      0.00      0.00         1\n",
            "          IN       1.00      1.00      1.00     21555\n",
            "          JJ       1.00      1.00      1.00     14181\n",
            "         JJR       1.00      1.00      1.00       530\n",
            "         JJS       1.00      1.00      1.00       540\n",
            "         LRB       1.00      1.00      1.00       146\n",
            "          MD       1.00      1.00      1.00      1248\n",
            "          NN       1.00      1.00      1.00     26009\n",
            "         NNP       1.00      1.00      1.00     23357\n",
            "        NNPS       1.00      1.00      1.00       451\n",
            "         NNS       1.00      1.00      1.00     13469\n",
            "         PDT       1.00      1.00      1.00        33\n",
            "         POS       1.00      1.00      1.00      2008\n",
            "         PRP       1.00      1.00      1.00      2342\n",
            "        PRP$       1.00      1.00      1.00      1576\n",
            "          RB       1.00      1.00      1.00      3601\n",
            "         RBR       1.00      1.00      1.00       198\n",
            "         RBS       1.00      1.00      1.00        46\n",
            "          RP       1.00      1.00      1.00       407\n",
            "         RRB       1.00      1.00      1.00       146\n",
            "          TO       1.00      1.00      1.00      4101\n",
            "          UH       1.00      1.00      1.00         6\n",
            "          VB       1.00      1.00      1.00      4230\n",
            "         VBD       1.00      1.00      1.00      6972\n",
            "         VBG       1.00      1.00      1.00      3367\n",
            "         VBN       1.00      1.00      1.00      5831\n",
            "         VBP       1.00      1.00      1.00      2844\n",
            "         VBZ       1.00      1.00      1.00      4421\n",
            "         WDT       1.00      1.00      1.00       652\n",
            "          WP       1.00      1.00      1.00       459\n",
            "         WP$       1.00      1.00      1.00        22\n",
            "         WRB       1.00      1.00      1.00       399\n",
            "          ``       1.00      1.00      1.00       711\n",
            "\n",
            "    accuracy                           1.00    186709\n",
            "   macro avg       0.98      0.98      0.98    186709\n",
            "weighted avg       1.00      1.00      1.00    186709\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
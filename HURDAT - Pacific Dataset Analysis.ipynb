{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "def solution():\n",
    "    \n",
    "    #******************* CHANGE FILE PATH ****************#\n",
    "    df_train=pd.read_csv('res/training/Pacific_train.csv')\n",
    "    df_test=pd.read_csv('res/validation/Pacific_test.csv')\n",
    "    #df_train=pd.read_csv(r'C:\\01 - Self Learnings\\AI ML\\Edureka\\Assignments\\03 - ML -Assignment 2 - MultiClass Classifiers\\Pacific_train.csv')\n",
    "    #df_test=pd.read_csv(r'C:\\01 - Self Learnings\\AI ML\\Edureka\\Assignments\\03 - ML -Assignment 2 - MultiClass Classifiers\\Pacific_test.csv')\n",
    "    #******************* CHANGE FILE PATH ****************#\n",
    "    \n",
    "    ########################################################################################################\n",
    "    ###### Data analysis\n",
    "    ########################################################################################################\n",
    "    #print('Data types in columns:\\n',df_train.dtypes,'\\n\\n\\n')\n",
    "    #print('Null Values analysis:\\n',df_train.isnull().sum(),'\\n\\n\\n')\n",
    "    \n",
    "    ########################################################################################################\n",
    "    ###### Drop unwanted object columns\n",
    "    ########################################################################################################\n",
    "    df_train = df_train.drop(['ID','Name','Date','Time','Latitude','Longitude'], axis = 1)\n",
    "    df_test = df_test.drop(['ID','Name','Date','Time','Latitude','Longitude'], axis = 1)\n",
    "    #print(df_train.head())\n",
    "    # Check pending datatypes\n",
    "    #print('Data types in columns:\\n',df_train.dtypes,'\\n\\n\\n')\n",
    "    \n",
    "    ########################################################################################################\n",
    "    ####### Label Encoding actual categorical data\n",
    "    ########################################################################################################\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le_event = LabelEncoder()\n",
    "    df_train['Event'] = le_event.fit_transform(df_train['Event'])\n",
    "    df_test['Event'] = le_event.transform(df_test['Event'])\n",
    "    \n",
    "    le_status = LabelEncoder()\n",
    "    df_train['Status'] = le_event.fit_transform(df_train['Status'])\n",
    "    df_test['Status'] = le_event.transform(df_test['Status'])\n",
    "    \n",
    "    #print(df_train.head())\n",
    "    # Check pending datatypes\n",
    "    #print('Data types in columns after removing unwanted columns and encoding:\\n',df_train.dtypes,'\\n\\n\\n')\n",
    "    \n",
    "    '''\n",
    "    ########################################################################################################\n",
    "    ###### Outlier Analysis and removal --> Not done because it removes more than 4000 rows\n",
    "    ########################################################################################################\n",
    "    old_shape = df_train.shape\n",
    "    my_list = df_train.iloc[:,2:].columns.values.tolist()\n",
    "    print(my_list)\n",
    "    \n",
    "    # Find Upper Bound and Lower Bound per column and store in a Dataframe - df_ub_lb\n",
    "    df_ub_lb = pd.DataFrame(columns = ['Column Name', 'Upper Bound', 'Lower Bound'])\n",
    "    row_num = 0\n",
    "    for x in my_list:\n",
    "        cur_col = df_train.loc[:,x]\n",
    "        cur_col_sorted = cur_col.sort_values()\n",
    "        iqr = cur_col_sorted.quantile(0.75) - cur_col_sorted.quantile(0.25)\n",
    "        upper_bound = cur_col_sorted.quantile(0.75) + (1.5 * iqr)\n",
    "        lower_bound = cur_col_sorted.quantile(0.25) - (1.5 * iqr)\n",
    "        my_row = [x,upper_bound,lower_bound]\n",
    "        df_ub_lb.loc[len(df_ub_lb)] = my_row\n",
    "    display(df_ub_lb)\n",
    "        \n",
    "    #Remove Rows beyond upper bound and lower bound\n",
    "    for x in my_list:\n",
    "        # Use the lb and ub dataframe fro previous for loop to find ub and lb for the specific columns\n",
    "        df_ub_lb_col = df_ub_lb.loc[df_ub_lb['Column Name'] == x]\n",
    "        df_ub_lb_col = df_ub_lb_col.reset_index()\n",
    "        lb = df_ub_lb_col.loc[0,'Lower Bound']\n",
    "        ub = df_ub_lb_col.loc[0,'Upper Bound']\n",
    "        # Remove data from the main dataframe\n",
    "        df_train.drop(df_train[df_train[x] > ub].index, inplace = True)\n",
    "        df_train.drop(df_train[df_train[x] < lb].index, inplace = True)\n",
    "        \n",
    "    new_shape = df_train.shape\n",
    "    print('Shape of original dataframe: ', old_shape)\n",
    "    print('Shape after removing outliers: ', new_shape)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Corelation Heat map --> Using this we see that all columns from Low Wind NE are strongly co-related\n",
    "    # So we can take only 1 from these\n",
    "    ########################################################################################################\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    matrix = np.triu(df_train.corr())\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    sns.heatmap(df_train.corr(), annot = True , vmin =-1 , vmax = 1, center = 0, cmap= 'coolwarm', \n",
    "                mask = matrix,linewidth=1, linecolor = 'black')\n",
    "    '''\n",
    "    \n",
    "    ########################################################################################################\n",
    "    #### Split data into X and y\n",
    "    #### As per hint in question, we take features as Maximum Wind, Minimum Pressure, Low Wind NE \n",
    "    ########################################################################################################\n",
    "    X_train = df_train.iloc[:,2:5]\n",
    "    y_train = pd.DataFrame(df_train.iloc[:,1], columns = ['Status'])\n",
    "    #print(X_train.head())\n",
    "    #print(y_train.head())\n",
    "    X_test = df_test.iloc[:,2:5]\n",
    "    y_test = pd.DataFrame(df_test.iloc[:,1], columns = ['Status'])\n",
    "    \n",
    "    ########################################################################################################\n",
    "    #### Apply Classification Models and their performance\n",
    "    ########################################################################################################\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score,precision_score\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    cross_val_dict = {}\n",
    "    df_performance = pd.DataFrame(columns = ['Model','Accuracy','Recall','Precision','F1-Score'])\n",
    "    \n",
    "    # Decision Tree - Entropy\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    dt_entropy = DecisionTreeClassifier(criterion='entropy')\n",
    "    cross_val_dict['DecisionTreeClassifier'] = cross_val_score(dt_entropy, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "    dt_entropy.fit(X_train,y_train)\n",
    "    y_pred_dt_entropy = dt_entropy.predict(X_test)\n",
    "    # Find performance values for each model\n",
    "    model = 'DecisionTreeClassifier'\n",
    "    accuracy = round(accuracy_score(y_test, y_pred_dt_entropy),2)\n",
    "    recall = recall_score(y_test, y_pred_dt_entropy, average = 'micro')\n",
    "    precision = precision_score(y_test, y_pred_dt_entropy, average = 'micro')\n",
    "    f1score = f1_score(y_test, y_pred_dt_entropy,average = 'micro')\n",
    "    # Create performance matrics rows\n",
    "    perf_row = [model,accuracy,recall,precision,f1score]\n",
    "    df_performance.loc[len(df_performance)] = perf_row\n",
    "    \n",
    "    # Decision Gini - Gini\n",
    "    dt_gini = DecisionTreeClassifier(criterion='gini')\n",
    "    cross_val_dict['DecisionTreeClassifier_Gini'] = cross_val_score(dt_gini, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "    dt_gini.fit(X_train,y_train)\n",
    "    y_pred_dt_gini = dt_gini.predict(X_test)\n",
    "    # Find performance values for each model\n",
    "    model = 'DecisionTreeClassifier_Gini'\n",
    "    accuracy = round(accuracy_score(y_test, y_pred_dt_gini),2)\n",
    "    recall = recall_score(y_test, y_pred_dt_gini,average = 'micro')\n",
    "    precision = precision_score(y_test, y_pred_dt_gini,average = 'micro')\n",
    "    f1score = f1_score(y_test, y_pred_dt_gini,average = 'micro')\n",
    "    # Create performance matrics rows\n",
    "    perf_row = [model,accuracy,recall,precision,f1score]\n",
    "    df_performance.loc[len(df_performance)] = perf_row\n",
    "    \n",
    "    # Random Forest\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rf_model = RandomForestClassifier()\n",
    "    cross_val_dict['RandomForestClassifier'] = cross_val_score(rf_model, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "    rf_model.fit(X_train,y_train)\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    # Find performance values for each model\n",
    "    model = 'RandomForestClassifier'\n",
    "    accuracy = round(accuracy_score(y_test, y_pred_rf),2)\n",
    "    recall = recall_score(y_test, y_pred_rf, average = 'micro')\n",
    "    precision = precision_score(y_test, y_pred_rf, average = 'micro')\n",
    "    f1score = f1_score(y_test, y_pred_rf,average = 'micro')\n",
    "    # Create performance matrics rows\n",
    "    perf_row = [model,accuracy,recall,precision,f1score]\n",
    "    df_performance.loc[len(df_performance)] = perf_row\n",
    "    \n",
    "    # Naive Bayes\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    nb_model = GaussianNB()\n",
    "    cross_val_dict['GaussianNB'] = cross_val_score(nb_model, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "    # Step 2 - Fit the Model\n",
    "    nb_model.fit(X_train,y_train)\n",
    "    # Step 3 - Predict from model\n",
    "    y_pred_test_nb = nb_model.predict(X_test)\n",
    "    # Find performance values for each model\n",
    "    model = 'GaussianNB'\n",
    "    accuracy = round(accuracy_score(y_test, y_pred_test_nb),2)\n",
    "    recall = recall_score(y_test, y_pred_test_nb, average = 'micro')\n",
    "    precision = precision_score(y_test, y_pred_test_nb,average = 'micro')\n",
    "    f1score = f1_score(y_test, y_pred_test_nb,average = 'micro')\n",
    "    # Create performance matrics rows\n",
    "    perf_row = [model,accuracy,recall,precision,f1score]\n",
    "    df_performance.loc[len(df_performance)] = perf_row\n",
    "    \n",
    "    #**** SVM Classifier Model\n",
    "    from sklearn.svm import SVC\n",
    "    # Step 1 - Call the model\n",
    "    svc_model = SVC()\n",
    "    cross_val_dict['SVC'] = cross_val_score(svc_model, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "    # Step 2 - Fit the model \n",
    "    svc_model.fit(X_train,y_train)\n",
    "    # Step 3 -  Predict from model\n",
    "    y_pred_test_svc = svc_model.predict(X_test)\n",
    "    # Find performance values for each model\n",
    "    model = 'SVC'\n",
    "    accuracy = round(accuracy_score(y_test, y_pred_test_svc),2)\n",
    "    recall = recall_score(y_test, y_pred_test_svc, average = 'micro')\n",
    "    precision = precision_score(y_test, y_pred_test_svc, average = 'micro')\n",
    "    f1score = f1_score(y_test, y_pred_test_svc, average = 'micro')\n",
    "    # Create performance matrics rows\n",
    "    perf_row = [model,accuracy,recall,precision,f1score]\n",
    "    df_performance.loc[len(df_performance)] = perf_row\n",
    "    \n",
    "    df_best = df_performance.loc[df_performance['Accuracy'] == df_performance['Accuracy'].max()]\n",
    "    df_output = df_best.iloc[:,0:2].T\n",
    "    \n",
    "    #print('Cross Val Scores: \\n',cross_val_dict,'\\n\\n')\n",
    "    #print('Performance of different models: \\n',df_performance)\n",
    "    \n",
    "    \n",
    "    #******************* CHANGE FILE PATH ****************#\n",
    "    #df_output.to_csv(r'C:\\01 - Self Learnings\\AI ML\\Edureka\\Assignments\\03 - ML -Assignment 2 - MultiClass Classifiers\\output.csv', index = False, header = False)\n",
    "    df_output.to_csv(r'output\\output.csv', index = False, header = False)\n",
    "    #******************* CHANGE FILE PATH ****************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
